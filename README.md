# ğŸ§ª LLM Inference Playground

A tool to explore how runtime parameters â€” like KV caching, precision, and batch size â€” affect the latency and memory usage of transformer models during inference.

## ğŸ” Features

- âœ… Toggle **KV caching** on/off
- âœ… Choose **precision**: FP32 / FP16 / BF16
- âœ… Adjust **sequence length** and **batch size**
- âœ… Track:
  - Latency per token
  - Total inference time
  - GPU memory usage
