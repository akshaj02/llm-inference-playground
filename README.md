# 🧪 LLM Inference Playground

A tool to explore how runtime parameters — like KV caching, precision, and batch size — affect the latency and memory usage of transformer models during inference.

## 🔍 Features

- ✅ Toggle **KV caching** on/off
- ✅ Choose **precision**: FP32 / FP16 / BF16
- ✅ Adjust **sequence length** and **batch size**
- ✅ Track:
  - Latency per token
  - Total inference time
  - GPU memory usage
